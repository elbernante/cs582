\documentclass{article}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

\usepackage{graphicx} % more modern
\usepackage{subcaption} 

% For citations
\usepackage{natbib}
\usepackage{url}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}

\usepackage{array}

% \usepackage{dblfloatfix}
% \usepackage{fixltx2e}


\usepackage[accepted]{icml2017}

\def\docTitle{Inception U-Net: Ultrasound Nerve Segmentation}

\icmltitlerunning{\docTitle}
\title{Inception U-Net}

\begin{document} 

\twocolumn[
\icmltitle{\docTitle}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2013
% package.

\begin{icmlauthorlist}
\icmlauthor{Peter James Bernante}{cs} \\
{\tt\small Maharishi University of Management} \\
{\tt\small pjbernante@mum.edu}
\end{icmlauthorlist}

\icmlaffiliation{cs}{Computer Science Department, Maharishi University of Management, Iowa, USA}

\icmlcorrespondingauthor{Peter James Bernante}{pjbernante@mum.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{healthcare, medical imaging, convolutional neural networks, ultrasound, brachial plexus, nerve segmentation, segmentation}
\vskip 0.3in

\input{abstract}
\bigskip
]
\printAffiliationsAndNotice{}

\input{intro}
\bigskip

\input{problem_formulation}
\bigskip

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.6\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/distribution_1.png}
        \caption{Classes are severely imbalanced}
        \label{fig:distribution_1}
    \end{subfigure}

   \begin{subfigure}[b]{0.7\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/distribution_2.png}
        \caption{Majority of the images have blank annotations (i.e. no annotated brachial plexus)}
        \label{fig:distribution_2}
    \end{subfigure}

    \caption{Distribution of classes}
    \label{fig:distribution}
\end{figure}

\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/high_corr.png}
  \caption{
      High correlation of images. The file names have the format \textless patient\_id\_xxx.tif\textgreater . Images coming from the same patient ID are highly correlated. (NOTE: The images are not exactly the same.)
  }
  \label{fig:high_corr}
\end{figure}

\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/inaccurate_1.png}
  \includegraphics[width=1.0\linewidth]{figures/inaccurate_2.png}
  \includegraphics[width=1.0\linewidth]{figures/inaccurate_3.png}
  \includegraphics[width=1.0\linewidth]{figures/inaccurate_4.png}
  \caption{
      Similar images with varying annotations. Human-annotated training images have inaccurate annotations.
  }
  \label{fig:inaccurate}
\end{figure}


\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/conflicting.png}
  \caption{
      Conflicting annotations. Very similar images have conflicting annotations. One image has the BP while the other has none. These are human errors during manual annotation of the dataset.
  }
  \label{fig:conflicting}
\end{figure}


\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/average_loc.png}
  \caption{
      Average annotations of similar images. Similar images have similar annotations, if present. The annotations do not exactly have the same shape, but they cover the same general area of the image and have common intersections.
  }
  \label{fig:average_loc}
\end{figure}

\begin{figure*}[ht]
 \centering
  \includegraphics[width=0.8\linewidth]{figures/inception-u-net.png}
  \caption{
      Inception U-Net Architecture. All layers in the middle uses Inception V3 and Grid Reduction modules instead of VGG and MaxPooling. Inception V4 is used in the coarsest layer at the bottom. Transpose Convolution is used to upsample the layer to the same size as the input. All Layers used SELU activation, with the exception of the last layer where Sigmoid was used for binary classification.
  }
  \label{fig:inception-u-net}
\end{figure*}




\input{data_exploration}
\bigskip

\input{preprocessing}
\bigskip


\input{model_architecture}
\bigskip

\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/loss.png}
  \caption{
      Training and Validation Loss
  }
  \label{fig:loss}
\end{figure}

\begin{figure}[h]
 \centering
  \includegraphics[width=1.0\linewidth]{figures/f1.png}
  \caption{
      Training and Validation $F_1$ Scores
  }
  \label{fig:f1}
\end{figure}

\input{training}
\bigskip


\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/result_1.png}
        \caption{Good performance of the model. The area highlighted in cyan is the ground truth while the area outlined in yellow is the prediction of the model. The model predictions are almost identical to the ground truth.}
        \label{fig:result_1}
    \end{subfigure}
    
    \begin{subfigure}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/result_2.png}
        \caption{Missed out predictions. From left to right: (a) the model prediction mostly missed the ground truth, (b) the model hit the ground truth but also annotated an extra area, (c) the model was not able to detect the brachial plexus where it actually existed, and (d) the model was able to find the brachial plexus where the human annotator was unable.}
        \label{fig:result_2}
    \end{subfigure}
    
    \begin{subfigure}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/result_3.png}
        \caption{Inference on test dataset. The yellow highlights are the prediction of the model. The test dataset has no accompanying ground truth to compare the results with. Consultation from trained ultrasound professional is needed.}
        \label{fig:result_3}
    \end{subfigure}


    \caption{Final Model Output}
    \label{fig:result}
\end{figure*}

\input{result}
\bigskip

\input{result_compare}
\bigskip

\input{conclusion}
\bigskip


\bibliographystyle{icml2017}
\bibliography{main}


\end{document} 

